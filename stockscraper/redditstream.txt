TITLE: Thermal Desktop Error when running steady stateTEXT: Read the error.  There are no boundary nodes or plenum.

Also do a print screen and don't use your phone...
Has anyone come across this error when running thermal desktop? The model will run in transient but not in steady state. It is a cubesat in orbit. I'm new to thermal desktop and still learning my way around it. Any help is appreciated!
They're not connected to anything. No loads, connections etc.
I guess that the mesh is not clean so these listed nodes do not connect to any element. What you can do is clean up the mesh, or manually delete these nodes.
TITLE: ABAQUS: Boundary conditions for a concrete pavementTEXT: *Disclaimer: This is for academic purposes.*

I'm trying to set up a model that isolates a single concrete slab (of a generic size, say 3x3m?). It's resting on some kind of idealized soil subgrade. My understanding is that most concrete floors (or pavements) are basically a bunch of concrete slabs laid together via joints, and have load transfer by dowels (and ties and what not).

An idealized subgrade seems simple enough: I'm going to make some deep block to simulate an infinite soil depth, fix the bottom and restrain the sides from lateral movement.

I'm having a little trouble with the concrete slab. I know I'll need a contact interaction between the slab and the subgrade. I'm pretty sure the slab should be allowed to lift off the subgrade (i.e.; if I have a bending moment or a point load in the middle).

The sides of the concrete slab is where I'm getting a little confused. If I'm modelling only a single concrete slab, there's going to be aggregate interlock and dowels that are meant to transfer load to adjacent slabs. In my heads this means I can't just assign a pin or a roller support at the ends of the slab and would instead require some kind of spring.

I know that if I have the edges unrestrained, then a point load in the center of the slab is going to cause the slab corner and edges to lift up. This wouldn't happen in real life because of dowels transferring load. If I pin or roller the edges of the slab, then the subgrade just deflects under it, leaving the ends of my slab floating in the air. 

Can anyone give me any direction on this? I have been going through research papers that have unrestrained concrete edges, pinned concrete edges, and roller concrete edges so I'm a little bit lost.Disclaimer: this may not what you want. Just model half other slabs at every edge of the original slab. Assign contact and boundary condition to the new part. Boundary condition is much simple now. This is similar to RVE technique.
Hey thanks for the reply!

I did consider this and from the literature I've been reading it appears a lot of models tend to look like this. 

I think my problem (and this might just be a lack of knowledge or understanding) is that I still wouldn't know what the boundary conditions of the new parts are. I get that it's a half slab so I'm just using symmetry, but since the slab is resting on some elastic block (my subgrade), I assume it would have to be able to deflect downwards with the soil if I applied some UDL on it, but also be free to deflect upwards if I impose a point load (and uplift occurs) or if I apply a bending moment (or any tensile force, really). 

I am currently considering idealizing the slab-slab interface just as a bunch of springs along the edge of the slab. This way, the slab shows some kind of "load transfer" no matter what load I impose onto it (i.e.; it's free to deflect with the soil or get yanked up). Do you think that would work?
Simply fixing lateral directions. The more slabs you add to the model the more accurate results. Just add more until your results converge. Yet, I guess deformation in the system should be small and concrete stiffness is high; thus, the error caused by boundary condition is somewhat higher at surrounding slabs, but not in the middle one.
TITLE: Need to do FEA on a 3D model but only have stl file. Help!TEXT: I need to conduct FEA on a 3D model, unfortunately I only have the stl file. I’m wondering how to go about converting it to a solid so that I can perform FEA on it. Any help with this is appreciated! Btw I have access to Solidworks, Fusion 360, Abaqus, and SpaceClaim. Thank you in advance, I’m very new to this!Don’t know any of those tools, but HyperMesh can fill the STL with solid elements. Quality will depend on the initial STL. 

I think SpaceClaim should have some tools to build this into a solid.
Spacelaim has some good tools. In the latest release they are in reverse engineering, and they generally refer to them as skinning.
If your initial stl is good quality then theres an option on abaqus called "convert tri to tet" in the mesh module
I remember doing this on Catia reconstructing an stl of a general aviation aircraft. I don’t think there were any tools available back then. But it wasn’t very hard if the geometry you want to model is straightforward.
Try cfmesh. Works in default on stls and will get you a great mesh. Used mostly for fluids. It’s also open source
TITLE: Using Nvidia K80 for Computing - Not such a good idea...TEXT: So I thought I could take advantage of the cheap NVidia Teslas K80 available on eBay to expedite FEA Analysis (Abaqus) and ordered a board. FYI, they are rated for about 2x1.7 TFlops in double precision and require dedicated cooling. The 2x is because there is in fact two logical boards in a single board package.

My computer is a Dell Precision 7810 with an 825W PSU, so I figured this should/would work. I figured that I would not be able crank all the CPUs to 100% \*and\* run the analysis because I did not have enough tokens anyway, and that power would not be an issue. The PCIe 3.0x16 slot actually stated 75W + 225W Max Power, so I figured I was in the clear (K80s are rated for 300 W).

The board was brand new and had never been used, with a manufacturing date from 2017... So I felt pretty good, but it went downhill from there. First of, the board uses a special cable, not the regular PCIe 8-pins Molex that you find on other GPUs, something similar but not quite. Once I figured that out (after ordering 4 different cables from Amazon) I could get my computer to start.

This is a passively cooled card, as in "your computer must provide sufficient air flow to cool it down". I figured that it was no biggie and designed something that I 3DPrinted to give me ample airflow. I could get the card to show up in Windows just fine, but the computer would crash while running Aida64 Benchmark (crash as in reboot with fan running full blast briefly). I started to monitor the temperatures, which hovered around 65ish degrees, up to 80ish degrees and figured that my airflow was probably insufficient (even though the board was supposed to throttle and not crash the CPU from what I could read). I ordered additional fans from Amazon and even hooked up a thermostat to the board to ensure that cooling would be sufficient. I did 4 additional iterations on that 3DPrinted shroud until I decided to use stronger 120 mm fans (finger eaters), remove the card built-in cover and my computer side cover.

Still. Every time, the Aida64 benchmark would crash during a particularly demanding test, even tough temperatures were max 50 degrees. I could run the benchmark on 1 board, but not on both of them. I installed the nvidia-smi utility which allows you to monitor power in real time to the board, and even throttle it to stay below a given power profile. I soon realised that the board was spiking around 320W during some of these tests, and figured that my 1400 VA UPS was perhaps on the weak side and plugged in directly in the wall. Still crashed during the same Aida64 test when running the benchmark on both boards (even though I limited the power requirements from each boards at 125 W). I gave up on the tests and decided to run it on Abaqus and start benchmarking it.

I asked my VAR to borrow additional Abaqus tokens and run some simple tests (in between crashes):

I wanted to see how effective GPGPU acceleration was. I ran the lap joint simulation (standard analysis with contacts, about 22444 elements - I know it is tiny - but was trying to get a feel.

I could not even get decent data out of it because the computer would keep crashing.

I returned the board to the seller on eBay who was very kind to accept my return. So, if you are looking for other means to crank up double precision performance, definitely look towards high end desktop-friendly boards + be careful because very few have high double precision performance (such as Nvidia Titan V, Nvidia Quadro GV100, Nvidia Titan Black or even Nvidia Quadro K6000).

TL,DR: Nvidia Tesla K80 have demanding power profile requirements which repeatedly crashed my computer.What is the Nvidia-recommended MB for the graphics card?
So rather than "K80s are bad for computing" you really mean, "K80s aren't meant for desktop computers, make sure you know what you're doing."  Otherwise, all those HPCs and supercomputers with K80s must be pretty bad for computing.
Mostly supermicro boards etc... Not the kind of stuff you can have under your desk since the relative high airflow is super noisy (think air dryer type flow). I can not locate a link from the Nvidia web site.
In other words yes. Thanks!
TITLE: Which option to pick to run my COMSOL file fast?TEXT: I have been trying to run COMSOL on my computer, which was too slow. I have been given instructions on how to run it on the cluster, I have never done this before and not sure which guide to follow:

What do the different highlighted options here mean? And which one do I most likely need to follow to run my COMSOL.mph files fast?

https://preview.redd.it/pgyj7vjyhrs61.png?width=507&format=png&auto=webp&s=588ce79e80d3001dfdd5d11a6274817229a3cc3b

Edit: Also, is there a difference between the highlighted options (blue line), and running comsol from the command line?Disclaimer: I'm not a COMSOL user, but I'm replying because I see no replies.

I think it depends on the hardware you're going to run it in. Find out the cluster's hardware first, read about the solve types second. Lastly, the difference between running a batch job and from its own command line should only be the GUI, not the result, but I'd ask someone with experience in COMSOL for that.
Check out this article for a discussion of the difference between shared/distributed/hybrid memory: https://www.comsol.com/blogs/hybrid-computing-advantages-shared-distributed-memory-combined/

Here is an easy to follow guide on how to set up cluster computing for the most common type of cluster: https://www.comsol.de/blogs/how-to-run-on-clusters-from-the-comsol-desktop-environment/ The details may vary depending on your hardware, OS and so on.

Here is a bit more detailed info on how to run Comsol on clusters: https://www.comsol.de/support/knowledgebase/1001

Note that not all models run faster on clusters. How much ram does your model need? Are you doing any sweeps? Is it time-dependent or nonlinear? Sometimes you also can speed up the model by tweaking the mesh or physics settings. What kind of hardware do you have that is too slow?
Am I on the right track?

[https://imgur.com/a/2o03Kw8](https://imgur.com/a/2o03Kw8)
I don't think I can answer that from the amount of info available to me, sorry.
You seem to be using LSF. See my other comment and follow the third link there. There is an example script for LSF on that page.

Your script is a good start, but has a few problems. It only uses one core, for example. But this is not really important. First, you need comsol to start computing your model at all, afterwards you can worry about settings. Start with a trivial model with like a cube and 10 mesh elements and only heat transfer or something, or use an example model. Swap in you real model, when everything else works.
